{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Extract Sample document and apply following document preprocessing methods:\n",
    "    Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "    2. Create representation of document by calculating Term Frequency and Inverse Document Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Natural Language Toolkit (nltk)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It was 7 minutes after midnight. The dog was lying on the grass in the middle of the lawn in front of Mrs Shears’ house. Its eyes were closed. It looked as if it was running on its side, the way dogs run when they think they are chasing a cat in a dream. But the dog was not running or asleep. The dog was dead. There was a garden fork sticking out of the dog. The points of the fork must have gone all the way through the dog and into the ground because the fork had not fallen over. I decided that the dog was probably killed with the fork because I could not see any other wounds in the dog and I do not think you would stick a garden fork into a dog after it had died for some other reason, like cancer for example, or a road accident. But I could not be certain about this.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting text from sample.txt\n",
    "file = open(\"/home/student/Documents/31170/A7/sample.txt\", \"rt\") \n",
    "text = file.read()     \n",
    "file.close()                   \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', '7', 'minutes', 'after', 'midnight.', 'The', 'dog', 'was', 'lying', 'on', 'the', 'grass', 'in', 'the', 'middle', 'of', 'the', 'lawn', 'in', 'front', 'of', 'Mrs', 'Shears’', 'house.', 'Its', 'eyes', 'were', 'closed.', 'It', 'looked', 'as', 'if', 'it', 'was', 'running', 'on', 'its', 'side,', 'the', 'way', 'dogs', 'run', 'when', 'they', 'think', 'they', 'are', 'chasing', 'a', 'cat', 'in', 'a', 'dream.', 'But', 'the', 'dog', 'was', 'not', 'running', 'or', 'asleep.', 'The', 'dog', 'was', 'dead.', 'There', 'was', 'a', 'garden', 'fork', 'sticking', 'out', 'of', 'the', 'dog.', 'The', 'points', 'of', 'the', 'fork', 'must', 'have', 'gone', 'all', 'the', 'way', 'through', 'the', 'dog', 'and', 'into', 'the', 'ground', 'because', 'the', 'fork', 'had', 'not', 'fallen', 'over.', 'I', 'decided', 'that', 'the', 'dog', 'was', 'probably', 'killed', 'with', 'the', 'fork', 'because', 'I', 'could', 'not', 'see', 'any', 'other', 'wounds', 'in', 'the', 'dog', 'and', 'I', 'do', 'not', 'think', 'you', 'would', 'stick', 'a', 'garden', 'fork', 'into', 'a', 'dog', 'after', 'it', 'had', 'died', 'for', 'some', 'other', 'reason,', 'like', 'cancer', 'for', 'example,', 'or', 'a', 'road', 'accident.', 'But', 'I', 'could', 'not', 'be', 'certain', 'about', 'this.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization by split()\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', '7', 'minutes', 'after', 'midnight', '.', 'The', 'dog', 'was', 'lying', 'on', 'the', 'grass', 'in', 'the', 'middle', 'of', 'the', 'lawn', 'in', 'front', 'of', 'Mrs', 'Shears', '’', 'house', '.', 'Its', 'eyes', 'were', 'closed', '.', 'It', 'looked', 'as', 'if', 'it', 'was', 'running', 'on', 'its', 'side', ',', 'the', 'way', 'dogs', 'run', 'when', 'they', 'think', 'they', 'are', 'chasing', 'a', 'cat', 'in', 'a', 'dream', '.', 'But', 'the', 'dog', 'was', 'not', 'running', 'or', 'asleep', '.', 'The', 'dog', 'was', 'dead', '.', 'There', 'was', 'a', 'garden', 'fork', 'sticking', 'out', 'of', 'the', 'dog', '.', 'The', 'points', 'of', 'the', 'fork', 'must', 'have', 'gone', 'all', 'the', 'way', 'through', 'the', 'dog', 'and', 'into', 'the', 'ground', 'because', 'the', 'fork', 'had', 'not', 'fallen', 'over', '.', 'I', 'decided', 'that', 'the', 'dog', 'was', 'probably', 'killed', 'with', 'the', 'fork', 'because', 'I', 'could', 'not', 'see', 'any', 'other', 'wounds', 'in', 'the', 'dog', 'and', 'I', 'do', 'not', 'think', 'you', 'would', 'stick', 'a', 'garden', 'fork', 'into', 'a', 'dog', 'after', 'it', 'had', 'died', 'for', 'some', 'other', 'reason', ',', 'like', 'cancer', 'for', 'example', ',', 'or', 'a', 'road', 'accident', '.', 'But', 'I', 'could', 'not', 'be', 'certain', 'about', 'this', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization by using nltk\n",
    "token = word_tokenize(text) \n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is a popular Natural Language Processing process which refers to categorizing words in a text in correspondence with a particular part of speech, depending on the definition of the word and its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>Abbreviation</th>\n",
    "        <th>Meaning</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>CC</td>\n",
    "        <td>Coordinating Conjunction</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>CD</td>\n",
    "        <td>Cardinal Digit</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>VB</td>\n",
    "        <td>verb (ask)</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>VBG</td>\n",
    "        <td>Verb Gerund (judging)</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>NNS</td>\n",
    "        <td>Noun Plural (desks)</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('It', 'PRP'), ('was', 'VBD'), ('7', 'CD'), ('minutes', 'NNS'), ('after', 'IN'), ('midnight', 'NN'), ('.', '.'), ('The', 'DT'), ('dog', 'NN'), ('was', 'VBD'), ('lying', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('grass', 'NN'), ('in', 'IN'), ('the', 'DT'), ('middle', 'NN'), ('of', 'IN'), ('the', 'DT'), ('lawn', 'NN'), ('in', 'IN'), ('front', 'NN'), ('of', 'IN'), ('Mrs', 'NNP'), ('Shears', 'NNP'), ('’', 'NNP'), ('house', 'NN'), ('.', '.'), ('Its', 'PRP$'), ('eyes', 'NNS'), ('were', 'VBD'), ('closed', 'VBN'), ('.', '.'), ('It', 'PRP'), ('looked', 'VBD'), ('as', 'IN'), ('if', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('running', 'VBG'), ('on', 'IN'), ('its', 'PRP$'), ('side', 'NN'), (',', ','), ('the', 'DT'), ('way', 'NN'), ('dogs', 'NNS'), ('run', 'VBP'), ('when', 'WRB'), ('they', 'PRP'), ('think', 'VBP'), ('they', 'PRP'), ('are', 'VBP'), ('chasing', 'VBG'), ('a', 'DT'), ('cat', 'NN'), ('in', 'IN'), ('a', 'DT'), ('dream', 'NN'), ('.', '.'), ('But', 'CC'), ('the', 'DT'), ('dog', 'NN'), ('was', 'VBD'), ('not', 'RB'), ('running', 'VBG'), ('or', 'CC'), ('asleep', 'NN'), ('.', '.'), ('The', 'DT'), ('dog', 'NN'), ('was', 'VBD'), ('dead', 'JJ'), ('.', '.'), ('There', 'EX'), ('was', 'VBD'), ('a', 'DT'), ('garden', 'NN'), ('fork', 'NN'), ('sticking', 'VBG'), ('out', 'IN'), ('of', 'IN'), ('the', 'DT'), ('dog', 'NN'), ('.', '.'), ('The', 'DT'), ('points', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('fork', 'NN'), ('must', 'MD'), ('have', 'VB'), ('gone', 'VBN'), ('all', 'PDT'), ('the', 'DT'), ('way', 'NN'), ('through', 'IN'), ('the', 'DT'), ('dog', 'NN'), ('and', 'CC'), ('into', 'IN'), ('the', 'DT'), ('ground', 'NN'), ('because', 'IN'), ('the', 'DT'), ('fork', 'NN'), ('had', 'VBD'), ('not', 'RB'), ('fallen', 'VBN'), ('over', 'IN'), ('.', '.'), ('I', 'PRP'), ('decided', 'VBD'), ('that', 'IN'), ('the', 'DT'), ('dog', 'NN'), ('was', 'VBD'), ('probably', 'RB'), ('killed', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('fork', 'NN'), ('because', 'IN'), ('I', 'PRP'), ('could', 'MD'), ('not', 'RB'), ('see', 'VB'), ('any', 'DT'), ('other', 'JJ'), ('wounds', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('dog', 'NN'), ('and', 'CC'), ('I', 'PRP'), ('do', 'VBP'), ('not', 'RB'), ('think', 'VB'), ('you', 'PRP'), ('would', 'MD'), ('stick', 'VB'), ('a', 'DT'), ('garden', 'NN'), ('fork', 'NN'), ('into', 'IN'), ('a', 'DT'), ('dog', 'NN'), ('after', 'IN'), ('it', 'PRP'), ('had', 'VBD'), ('died', 'VBN'), ('for', 'IN'), ('some', 'DT'), ('other', 'JJ'), ('reason', 'NN'), (',', ','), ('like', 'IN'), ('cancer', 'NN'), ('for', 'IN'), ('example', 'NN'), (',', ','), ('or', 'CC'), ('a', 'DT'), ('road', 'NN'), ('accident', 'NN'), ('.', '.'), ('But', 'CC'), ('I', 'PRP'), ('could', 'MD'), ('not', 'RB'), ('be', 'VB'), ('certain', 'JJ'), ('about', 'IN'), ('this', 'DT'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Parts of Speech (POS) Tagging using nltk\n",
    "tagged = pos_tag(token)      \n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words which are generally filtered out before processing a natural language are called stop words. These are actually the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text. Examples of a few stop words in English are “the”, “a”, “an”, “so”, “what”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable stop_words to store all of the stop words in the English language using nltk\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the word is part of stop_words and adding it to cleaned_token if it is not\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclean version: ['It', 'was', '7', 'minutes', 'after', 'midnight', '.', 'The', 'dog', 'was', 'lying', 'on', 'the', 'grass', 'in', 'the', 'middle', 'of', 'the', 'lawn', 'in', 'front', 'of', 'Mrs', 'Shears', '’', 'house', '.', 'Its', 'eyes', 'were', 'closed', '.', 'It', 'looked', 'as', 'if', 'it', 'was', 'running', 'on', 'its', 'side', ',', 'the', 'way', 'dogs', 'run', 'when', 'they', 'think', 'they', 'are', 'chasing', 'a', 'cat', 'in', 'a', 'dream', '.', 'But', 'the', 'dog', 'was', 'not', 'running', 'or', 'asleep', '.', 'The', 'dog', 'was', 'dead', '.', 'There', 'was', 'a', 'garden', 'fork', 'sticking', 'out', 'of', 'the', 'dog', '.', 'The', 'points', 'of', 'the', 'fork', 'must', 'have', 'gone', 'all', 'the', 'way', 'through', 'the', 'dog', 'and', 'into', 'the', 'ground', 'because', 'the', 'fork', 'had', 'not', 'fallen', 'over', '.', 'I', 'decided', 'that', 'the', 'dog', 'was', 'probably', 'killed', 'with', 'the', 'fork', 'because', 'I', 'could', 'not', 'see', 'any', 'other', 'wounds', 'in', 'the', 'dog', 'and', 'I', 'do', 'not', 'think', 'you', 'would', 'stick', 'a', 'garden', 'fork', 'into', 'a', 'dog', 'after', 'it', 'had', 'died', 'for', 'some', 'other', 'reason', ',', 'like', 'cancer', 'for', 'example', ',', 'or', 'a', 'road', 'accident', '.', 'But', 'I', 'could', 'not', 'be', 'certain', 'about', 'this', '.']\n",
      "\n",
      "Cleaned version: ['It', '7', 'minutes', 'midnight', '.', 'The', 'dog', 'lying', 'grass', 'middle', 'lawn', 'front', 'Mrs', 'Shears', '’', 'house', '.', 'Its', 'eyes', 'closed', '.', 'It', 'looked', 'running', 'side', ',', 'way', 'dogs', 'run', 'think', 'chasing', 'cat', 'dream', '.', 'But', 'dog', 'running', 'asleep', '.', 'The', 'dog', 'dead', '.', 'There', 'garden', 'fork', 'sticking', 'dog', '.', 'The', 'points', 'fork', 'must', 'gone', 'way', 'dog', 'ground', 'fork', 'fallen', '.', 'I', 'decided', 'dog', 'probably', 'killed', 'fork', 'I', 'could', 'see', 'wounds', 'dog', 'I', 'think', 'would', 'stick', 'garden', 'fork', 'dog', 'died', 'reason', ',', 'like', 'cancer', 'example', ',', 'road', 'accident', '.', 'But', 'I', 'could', 'certain', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Unclean version:', token)\n",
    "print('\\nCleaned version:', cleaned_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Stemming to remove suffixes from words and end up with a so-called word stem. The words “likes”, “likely” and “liked”, for example, all result in their common word stem “like” which can be used as a synonym for all three words. That way, an NLP model can learn that all three words are somehow similar and are used in a similar context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter's Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porter’s Stemmer Algorithm is one of the most popular Stemming methods and was proposed in 1980. It is based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes. It is known for its efficient and simple processes, but also comes with several disadvantages. Since it is based on many, hard-coded rules which result from the English language, it can only be used for English words. Also, there may be cases in which the output of Porter’s Stemmer is not an English word but only an artificial word stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it wa 7 minut after midnight . the dog wa lie on the grass in the middl of the lawn in front of mr shear ’ hous . it eye were close . it look as if it wa run on it side , the way dog run when they think they are chase a cat in a dream . but the dog wa not run or asleep . the dog wa dead . there wa a garden fork stick out of the dog . the point of the fork must have gone all the way through the dog and into the ground becaus the fork had not fallen over . i decid that the dog wa probabl kill with the fork becaus i could not see ani other wound in the dog and i do not think you would stick a garden fork into a dog after it had die for some other reason , like cancer for exampl , or a road accid . but i could not be certain about thi .\n"
     ]
    }
   ],
   "source": [
    "# Using Port Stemmer in nltk for stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in token]\n",
    "print(\" \".join(stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a development of Stemming and describes the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to Stemming but it brings context to the words. So it links words with similar meanings to one word. Lemmatization algorithms usually also use positional arguments as inputs, such as whether the word is an adjective, noun, or verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It wa 7 minute after midnight . The dog wa lying on the grass in the middle of the lawn in front of Mrs Shears ’ house . Its eye were closed . It looked a if it wa running on it side , the way dog run when they think they are chasing a cat in a dream . But the dog wa not running or asleep . The dog wa dead . There wa a garden fork sticking out of the dog . The point of the fork must have gone all the way through the dog and into the ground because the fork had not fallen over . I decided that the dog wa probably killed with the fork because I could not see any other wound in the dog and I do not think you would stick a garden fork into a dog after it had died for some other reason , like cancer for example , or a road accident . But I could not be certain about this .\n"
     ]
    }
   ],
   "source": [
    "# Performing lemmaization using nltk\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_output = [lemmatizer.lemmatize(word) for word in token]\n",
    "print(\" \".join(lemmatized_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency (TF) and Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Term frequency</b> is the number of times a term occurs in a document.The weight of a term that occurs in a document is simply proportional to the term frequency.\n",
    "\n",
    "Formula: tf(t,d) = count of t in d / number of words in d\n",
    "\n",
    "<b>Inverse document frequency</b> is the inverse of the document frequency which measures the informativeness of term t. In case of a large corpus,say 100,000,000, the IDF value explodes, to avoid the effect we take the log of idf.\n",
    "\n",
    "Formula: idf(t) = log(N/(df + 1))\n",
    "\n",
    "<b>TF-IDF</b> now is a the right measure to evaluate how important a word is to a document in a collection or corpus.\n",
    "\n",
    "Formula: tf-idf(t, d) = tf(t, d) * log(N/(df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting string object to raw text format\n",
    "text = [text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn's TfidfVectorizer() function to find term frequency and inverse term frequency\n",
    "vectorize = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 77)\t0.03984095364447979\n",
      "  (0, 0)\t0.03984095364447979\n",
      "  (0, 14)\t0.03984095364447979\n",
      "  (0, 9)\t0.03984095364447979\n",
      "  (0, 1)\t0.03984095364447979\n",
      "  (0, 63)\t0.03984095364447979\n",
      "  (0, 25)\t0.03984095364447979\n",
      "  (0, 12)\t0.03984095364447979\n",
      "  (0, 45)\t0.03984095364447979\n",
      "  (0, 62)\t0.03984095364447979\n",
      "  (0, 69)\t0.03984095364447979\n",
      "  (0, 28)\t0.07968190728895957\n",
      "  (0, 20)\t0.03984095364447979\n",
      "  (0, 70)\t0.03984095364447979\n",
      "  (0, 84)\t0.03984095364447979\n",
      "  (0, 86)\t0.03984095364447979\n",
      "  (0, 21)\t0.03984095364447979\n",
      "  (0, 85)\t0.03984095364447979\n",
      "  (0, 57)\t0.07968190728895957\n",
      "  (0, 5)\t0.03984095364447979\n",
      "  (0, 66)\t0.03984095364447979\n",
      "  (0, 17)\t0.07968190728895957\n",
      "  (0, 83)\t0.03984095364447979\n",
      "  (0, 43)\t0.03984095364447979\n",
      "  (0, 61)\t0.03984095364447979\n",
      "  :\t:\n",
      "  (0, 38)\t0.03984095364447979\n",
      "  (0, 7)\t0.03984095364447979\n",
      "  (0, 46)\t0.03984095364447979\n",
      "  (0, 16)\t0.03984095364447979\n",
      "  (0, 81)\t0.03984095364447979\n",
      "  (0, 26)\t0.03984095364447979\n",
      "  (0, 42)\t0.07968190728895957\n",
      "  (0, 37)\t0.03984095364447979\n",
      "  (0, 67)\t0.03984095364447979\n",
      "  (0, 51)\t0.03984095364447979\n",
      "  (0, 30)\t0.03984095364447979\n",
      "  (0, 44)\t0.03984095364447979\n",
      "  (0, 54)\t0.15936381457791915\n",
      "  (0, 48)\t0.03984095364447979\n",
      "  (0, 39)\t0.15936381457791915\n",
      "  (0, 33)\t0.03984095364447979\n",
      "  (0, 55)\t0.07968190728895957\n",
      "  (0, 47)\t0.03984095364447979\n",
      "  (0, 22)\t0.3187276291558383\n",
      "  (0, 73)\t0.6772962119561564\n",
      "  (0, 49)\t0.03984095364447979\n",
      "  (0, 2)\t0.07968190728895957\n",
      "  (0, 50)\t0.03984095364447979\n",
      "  (0, 79)\t0.27888667551135854\n",
      "  (0, 41)\t0.15936381457791915\n"
     ]
    }
   ],
   "source": [
    "# Fitting the model and passing our text\n",
    "response = vectorize.fit_transform(text)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
