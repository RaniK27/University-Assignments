{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Extract Sample document and apply following document preprocessing methods:\n",
    "    Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "    2. Create representation of document by calculating Term Frequency and Inverse Document Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Natural Language Toolkit (nltk)\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two roads diverged in a yellow wood,\n",
      "And sorry I could not travel both\n",
      "And be one traveler, long I stood\n",
      "And looked down one as far as I could\n",
      "To where it bent in the undergrowth;\n",
      "\n",
      "Then took the other, as just as fair,\n",
      "And having perhaps the better claim,\n",
      "Because it was grassy and wanted wear;\n",
      "Though as for that the passing there\n",
      "Had worn them really about the same,\n",
      "\n",
      "And both that morning equally lay\n",
      "In leaves no step had trodden black.\n",
      "Oh, I kept the first for another day!\n",
      "Yet knowing how way leads on to way,\n",
      "I doubted if I should ever come back.\n",
      "\n",
      "I shall be telling this with a sigh\n",
      "Somewhere ages and ages hence:\n",
      "Two roads diverged in a wood, and I—\n",
      "I took the one less traveled by,\n",
      "And that has made all the difference.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting text from sample.txt\n",
    "file = open(\"/home/student/Documents/31170/A7/sample.txt\", \"rt\") \n",
    "text = file.read()        \n",
    "file.close()                   \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two', 'roads', 'diverged', 'in', 'a', 'yellow', 'wood,', 'And', 'sorry', 'I', 'could', 'not', 'travel', 'both', 'And', 'be', 'one', 'traveler,', 'long', 'I', 'stood', 'And', 'looked', 'down', 'one', 'as', 'far', 'as', 'I', 'could', 'To', 'where', 'it', 'bent', 'in', 'the', 'undergrowth;', 'Then', 'took', 'the', 'other,', 'as', 'just', 'as', 'fair,', 'And', 'having', 'perhaps', 'the', 'better', 'claim,', 'Because', 'it', 'was', 'grassy', 'and', 'wanted', 'wear;', 'Though', 'as', 'for', 'that', 'the', 'passing', 'there', 'Had', 'worn', 'them', 'really', 'about', 'the', 'same,', 'And', 'both', 'that', 'morning', 'equally', 'lay', 'In', 'leaves', 'no', 'step', 'had', 'trodden', 'black.', 'Oh,', 'I', 'kept', 'the', 'first', 'for', 'another', 'day!', 'Yet', 'knowing', 'how', 'way', 'leads', 'on', 'to', 'way,', 'I', 'doubted', 'if', 'I', 'should', 'ever', 'come', 'back.', 'I', 'shall', 'be', 'telling', 'this', 'with', 'a', 'sigh', 'Somewhere', 'ages', 'and', 'ages', 'hence:', 'Two', 'roads', 'diverged', 'in', 'a', 'wood,', 'and', 'I—', 'I', 'took', 'the', 'one', 'less', 'traveled', 'by,', 'And', 'that', 'has', 'made', 'all', 'the', 'difference.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization by split()\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two', 'roads', 'diverged', 'in', 'a', 'yellow', 'wood', ',', 'And', 'sorry', 'I', 'could', 'not', 'travel', 'both', 'And', 'be', 'one', 'traveler', ',', 'long', 'I', 'stood', 'And', 'looked', 'down', 'one', 'as', 'far', 'as', 'I', 'could', 'To', 'where', 'it', 'bent', 'in', 'the', 'undergrowth', ';', 'Then', 'took', 'the', 'other', ',', 'as', 'just', 'as', 'fair', ',', 'And', 'having', 'perhaps', 'the', 'better', 'claim', ',', 'Because', 'it', 'was', 'grassy', 'and', 'wanted', 'wear', ';', 'Though', 'as', 'for', 'that', 'the', 'passing', 'there', 'Had', 'worn', 'them', 'really', 'about', 'the', 'same', ',', 'And', 'both', 'that', 'morning', 'equally', 'lay', 'In', 'leaves', 'no', 'step', 'had', 'trodden', 'black', '.', 'Oh', ',', 'I', 'kept', 'the', 'first', 'for', 'another', 'day', '!', 'Yet', 'knowing', 'how', 'way', 'leads', 'on', 'to', 'way', ',', 'I', 'doubted', 'if', 'I', 'should', 'ever', 'come', 'back', '.', 'I', 'shall', 'be', 'telling', 'this', 'with', 'a', 'sigh', 'Somewhere', 'ages', 'and', 'ages', 'hence', ':', 'Two', 'roads', 'diverged', 'in', 'a', 'wood', ',', 'and', 'I—', 'I', 'took', 'the', 'one', 'less', 'traveled', 'by', ',', 'And', 'that', 'has', 'made', 'all', 'the', 'difference', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization by using nltk\n",
    "token = word_tokenize(text) \n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is a popular Natural Language Processing process which refers to categorizing words in a text in correspondence with a particular part of speech, depending on the definition of the word and its context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>Abbreviation</th>\n",
    "        <th>Meaning</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>CC</td>\n",
    "        <td>Coordinating Conjunction</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>CD</td>\n",
    "        <td>Cardinal Digit</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>VB</td>\n",
    "        <td>verb (ask)</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>VBG</td>\n",
    "        <td>Verb Gerund (judging)</td>\n",
    "    </tr>\n",
    "     <tr>\n",
    "        <td>NNS</td>\n",
    "        <td>Noun Plural (desks)</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Two', 'CD'), ('roads', 'NNS'), ('diverged', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('yellow', 'JJ'), ('wood', 'NN'), (',', ','), ('And', 'CC'), ('sorry', 'NN'), ('I', 'PRP'), ('could', 'MD'), ('not', 'RB'), ('travel', 'VB'), ('both', 'DT'), ('And', 'CC'), ('be', 'VB'), ('one', 'CD'), ('traveler', 'NN'), (',', ','), ('long', 'RB'), ('I', 'PRP'), ('stood', 'VBD'), ('And', 'CC'), ('looked', 'VBD'), ('down', 'RB'), ('one', 'CD'), ('as', 'RB'), ('far', 'RB'), ('as', 'IN'), ('I', 'PRP'), ('could', 'MD'), ('To', 'TO'), ('where', 'WRB'), ('it', 'PRP'), ('bent', 'VBD'), ('in', 'IN'), ('the', 'DT'), ('undergrowth', 'NN'), (';', ':'), ('Then', 'RB'), ('took', 'VBD'), ('the', 'DT'), ('other', 'JJ'), (',', ','), ('as', 'RB'), ('just', 'RB'), ('as', 'IN'), ('fair', 'JJ'), (',', ','), ('And', 'CC'), ('having', 'VBG'), ('perhaps', 'RB'), ('the', 'DT'), ('better', 'JJR'), ('claim', 'NN'), (',', ','), ('Because', 'IN'), ('it', 'PRP'), ('was', 'VBD'), ('grassy', 'JJ'), ('and', 'CC'), ('wanted', 'VBD'), ('wear', 'NN'), (';', ':'), ('Though', 'NNP'), ('as', 'IN'), ('for', 'IN'), ('that', 'IN'), ('the', 'DT'), ('passing', 'NN'), ('there', 'RB'), ('Had', 'NNP'), ('worn', 'VBN'), ('them', 'PRP'), ('really', 'RB'), ('about', 'IN'), ('the', 'DT'), ('same', 'JJ'), (',', ','), ('And', 'CC'), ('both', 'DT'), ('that', 'IN'), ('morning', 'NN'), ('equally', 'RB'), ('lay', 'VBD'), ('In', 'IN'), ('leaves', 'VBZ'), ('no', 'DT'), ('step', 'NN'), ('had', 'VBD'), ('trodden', 'JJ'), ('black', 'JJ'), ('.', '.'), ('Oh', 'UH'), (',', ','), ('I', 'PRP'), ('kept', 'VBD'), ('the', 'DT'), ('first', 'JJ'), ('for', 'IN'), ('another', 'DT'), ('day', 'NN'), ('!', '.'), ('Yet', 'RB'), ('knowing', 'VBG'), ('how', 'WRB'), ('way', 'NN'), ('leads', 'VBZ'), ('on', 'IN'), ('to', 'TO'), ('way', 'NN'), (',', ','), ('I', 'PRP'), ('doubted', 'VBD'), ('if', 'IN'), ('I', 'PRP'), ('should', 'MD'), ('ever', 'RB'), ('come', 'VB'), ('back', 'RB'), ('.', '.'), ('I', 'PRP'), ('shall', 'MD'), ('be', 'VB'), ('telling', 'VBG'), ('this', 'DT'), ('with', 'IN'), ('a', 'DT'), ('sigh', 'NN'), ('Somewhere', 'NNP'), ('ages', 'NNS'), ('and', 'CC'), ('ages', 'NNS'), ('hence', 'RB'), (':', ':'), ('Two', 'CD'), ('roads', 'NNS'), ('diverged', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('wood', 'NN'), (',', ','), ('and', 'CC'), ('I—', 'NNP'), ('I', 'PRP'), ('took', 'VBD'), ('the', 'DT'), ('one', 'NN'), ('less', 'JJR'), ('traveled', 'VBN'), ('by', 'IN'), (',', ','), ('And', 'CC'), ('that', 'DT'), ('has', 'VBZ'), ('made', 'VBN'), ('all', 'PDT'), ('the', 'DT'), ('difference', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Parts of Speech (POS) Tagging using nltk\n",
    "tagged = pos_tag(token)      \n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words which are generally filtered out before processing a natural language are called stop words. These are actually the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text. Examples of a few stop words in English are “the”, “a”, “an”, “so”, “what”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a variable stop_words to store all of the stop words in the English language using nltk\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the word is part of stop_words and adding it to cleaned_token if it is not\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclean version: ['Two', 'roads', 'diverged', 'in', 'a', 'yellow', 'wood', ',', 'And', 'sorry', 'I', 'could', 'not', 'travel', 'both', 'And', 'be', 'one', 'traveler', ',', 'long', 'I', 'stood', 'And', 'looked', 'down', 'one', 'as', 'far', 'as', 'I', 'could', 'To', 'where', 'it', 'bent', 'in', 'the', 'undergrowth', ';', 'Then', 'took', 'the', 'other', ',', 'as', 'just', 'as', 'fair', ',', 'And', 'having', 'perhaps', 'the', 'better', 'claim', ',', 'Because', 'it', 'was', 'grassy', 'and', 'wanted', 'wear', ';', 'Though', 'as', 'for', 'that', 'the', 'passing', 'there', 'Had', 'worn', 'them', 'really', 'about', 'the', 'same', ',', 'And', 'both', 'that', 'morning', 'equally', 'lay', 'In', 'leaves', 'no', 'step', 'had', 'trodden', 'black', '.', 'Oh', ',', 'I', 'kept', 'the', 'first', 'for', 'another', 'day', '!', 'Yet', 'knowing', 'how', 'way', 'leads', 'on', 'to', 'way', ',', 'I', 'doubted', 'if', 'I', 'should', 'ever', 'come', 'back', '.', 'I', 'shall', 'be', 'telling', 'this', 'with', 'a', 'sigh', 'Somewhere', 'ages', 'and', 'ages', 'hence', ':', 'Two', 'roads', 'diverged', 'in', 'a', 'wood', ',', 'and', 'I—', 'I', 'took', 'the', 'one', 'less', 'traveled', 'by', ',', 'And', 'that', 'has', 'made', 'all', 'the', 'difference', '.']\n",
      "\n",
      "Cleaned version: ['Two', 'roads', 'diverged', 'yellow', 'wood', ',', 'And', 'sorry', 'I', 'could', 'travel', 'And', 'one', 'traveler', ',', 'long', 'I', 'stood', 'And', 'looked', 'one', 'far', 'I', 'could', 'To', 'bent', 'undergrowth', ';', 'Then', 'took', ',', 'fair', ',', 'And', 'perhaps', 'better', 'claim', ',', 'Because', 'grassy', 'wanted', 'wear', ';', 'Though', 'passing', 'Had', 'worn', 'really', ',', 'And', 'morning', 'equally', 'lay', 'In', 'leaves', 'step', 'trodden', 'black', '.', 'Oh', ',', 'I', 'kept', 'first', 'another', 'day', '!', 'Yet', 'knowing', 'way', 'leads', 'way', ',', 'I', 'doubted', 'I', 'ever', 'come', 'back', '.', 'I', 'shall', 'telling', 'sigh', 'Somewhere', 'ages', 'ages', 'hence', ':', 'Two', 'roads', 'diverged', 'wood', ',', 'I—', 'I', 'took', 'one', 'less', 'traveled', ',', 'And', 'made', 'difference', '.']\n"
     ]
    }
   ],
   "source": [
    "print('Unclean version:', token)\n",
    "print('\\nCleaned version:', cleaned_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Stemming to remove suffixes from words and end up with a so-called word stem. The words “likes”, “likely” and “liked”, for example, all result in their common word stem “like” which can be used as a synonym for all three words. That way, an NLP model can learn that all three words are somehow similar and are used in a similar context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porter's Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porter’s Stemmer Algorithm is one of the most popular Stemming methods and was proposed in 1980. It is based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes. It is known for its efficient and simple processes, but also comes with several disadvantages. Since it is based on many, hard-coded rules which result from the English language, it can only be used for English words. Also, there may be cases in which the output of Porter’s Stemmer is not an English word but only an artificial word stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two road diverg in a yellow wood , and sorri i could not travel both and be one travel , long i stood and look down one as far as i could to where it bent in the undergrowth ; then took the other , as just as fair , and have perhap the better claim , becaus it wa grassi and want wear ; though as for that the pass there had worn them realli about the same , and both that morn equal lay in leav no step had trodden black . oh , i kept the first for anoth day ! yet know how way lead on to way , i doubt if i should ever come back . i shall be tell thi with a sigh somewher age and age henc : two road diverg in a wood , and i— i took the one less travel by , and that ha made all the differ .\n"
     ]
    }
   ],
   "source": [
    "# Using Port Stemmer in nltk for stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in token]\n",
    "print(\" \".join(stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is a development of Stemming and describes the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to Stemming but it brings context to the words. So it links words with similar meanings to one word. Lemmatization algorithms usually also use positional arguments as inputs, such as whether the word is an adjective, noun, or verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two road diverged in a yellow wood , And sorry I could not travel both And be one traveler , long I stood And looked down one a far a I could To where it bent in the undergrowth ; Then took the other , a just a fair , And having perhaps the better claim , Because it wa grassy and wanted wear ; Though a for that the passing there Had worn them really about the same , And both that morning equally lay In leaf no step had trodden black . Oh , I kept the first for another day ! Yet knowing how way lead on to way , I doubted if I should ever come back . I shall be telling this with a sigh Somewhere age and age hence : Two road diverged in a wood , and I— I took the one le traveled by , And that ha made all the difference .\n"
     ]
    }
   ],
   "source": [
    "# Performing lemmaization using nltk\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_output = [lemmatizer.lemmatize(word) for word in token]\n",
    "print(\" \".join(lemmatized_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
